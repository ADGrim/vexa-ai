import React, { useState } from 'react';

const VexaVoiceListener = ({ onVexaRespond }) => {
  const [listening, setListening] = useState(false);

  const speakWithNova = async (text) => {
    const response = await fetch("https://api.openai.com/v1/audio/speech", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer YOUR_OPENAI_API_KEY`,
      },
      body: JSON.stringify({
        model: "tts-1",
        voice: "nova",
        input: text,
      }),
    });

    const audioBlob = await response.blob();
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);
    audio.play();
  };

  const startListening = () => {
    const recognition = new window.webkitSpeechRecognition() || new window.SpeechRecognition();
    recognition.continuous = false;
    recognition.lang = 'en-US';
    recognition.interimResults = false;

    recognition.start();
    setListening(true);

    recognition.onresult = async (event) => {
      const userSpeech = event.results[0][0].transcript;
      console.log("You said:", userSpeech);

      // You can send userSpeech to OpenAI or your response generator
      const responseText = await onVexaRespond(userSpeech);
      speakWithNova(responseText);
      setListening(false);
    };

    recognition.onerror = (err) => {
      console.error("Speech recognition error:", err);
      setListening(false);
    };

    recognition.onend = () => {
      setListening(false);
    };
  };

  return (
    <div
      onClick={startListening}
      className="cursor-pointer hover:scale-105 transition-transform flex flex-col items-center"
    >
      <img
        src="/wave-button.png"
        alt="Vexa Listening Button"
        className={`w-28 h-28 ${listening ? 'animate-pulse' : ''}`}
      />
      <p className="text-center text-indigo-600 font-semibold mt-2">
        {listening ? "Listening..." : "Talk to Vexa"}
      </p>
    </div>
  );
};

export default VexaVoiceListener;
